			+--------------------+
			|        EE 415      |
			| PROJECT 1: THREADS |
			|   DESIGN DOCUMENT  |
			+--------------------+
				   
---- GROUP ----

>> Fill in the names and email addresses of your group members.

Celso Luiz Barroso de Moraes <asura254@kaist.ac.kr>
Changhwan Kim <kimbob@kaist.ac.kr>

---- PRELIMINARIES ----

>> If you have any preliminary comments on your submission, notes for the
>> TAs, or extra credit, please give them here.

>> Please cite any offline or online sources you consulted while
>> preparing your submission, other than the Pintos documentation, course
>> text, lecture notes, and course staff.

			     ALARM CLOCK
			     ===========

---- DATA STRUCTURES ----

>> A1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

////

struct thread
  { ....

    /* Alarm clock */
    struct list_elem sleep_elem;      /* Sleep list element */
    int64_t wake_time;                /* Wake up time for this thread */
....}

wake_time: As the name suggests, it stores the time that each thread will be sleeping for.
sleep_elem:List elem to catalog all the threads that are sleeping. This list is ordered by priority. 

////

static struct list sleep_list;
list to store the sleeping elements.

////

static int64_t next_wakeup_time;

variable to store the wake up time

////

---- ALGORITHMS ----

>> A2: Briefly describe what happens in a call to timer_sleep(),
>> including the effects of the timer interrupt handler.

When the function timer_sleep() is called, the thread element wake_up time which 
was defined by us, will increment to the desired time value. Afterwards, the thread is going to be added
to the sleep list by the function add_to_sleep_list. After the add_to_sleep_list returns the current running thread is blockedby the thread_block() routine.
The functions will be woken up by another defined function called check_wake_up_time(), which is called by the interrupt
handler. The correct order to wake up the functions is going to be defined by the thread_unblock() function, which was
modified to use the list_inser_ordered function instead, and use the priority of the threads as ther ordering element.
The ordering will be perofmed by the priority_comp function defined by us. 


>> A3: What steps are taken to minimize the amount of time spent in
>> the timer interrupt handler?

Inside the timer interrupt the only thing that will happen will be the calling of the function check_wake_time, which
as explained in the previous answer is the responsible for waking up the threads once the sleep time is over. 

---- SYNCHRONIZATION ----

>> A4: How are race conditions avoided when multiple threads call
>> timer_sleep() simultaneously?
Race in conditions for our function wont be much of a problem, since upon entering the timer sleep function the interrupts
are disable for the thread_unblock function call. Whichever thread enters first is going to be scheduled in relation to 
its priority, so the outcome will always be deterministic no matter the order that the threads enter. 

>> A5: How are race conditions avoided when a timer interrupt occurs
>> during a call to timer_sleep()?
Since interrupts are disable in the beginning of the timer_sleep function than this situation would not be possible to occur,
therefore we do not suffer from such issue. The only reason that we disabled interrupt was to be able to call our modified thread_block function.
---- RATIONALE ----

>> A6: Why did you choose this design?  In what ways is it superior to
>> another design you considered?
There were 2 designs that were thought of by each of the group members, and in the end we chose one design to submit. Another
design that was suggested was utilizing a while loop as before, but instead of busy running like the original implementation,
it would run in the idle function. A function called lock_away (commented out in the thread.c file) would be responsible
for locking each thread in a order of arrival method. The correct priority order would be set up by a second function which 
would be responsible for awakening the thread of highest priority in the sleeping least every time it was called. We opted to
not use this method for our submission because we thought that running a while loop in the  idle thread might characterize some form of busy waiting and not a totally idle processor as it was desired.


			 PRIORITY SCHEDULING
			 ===================

---- DATA STRUCTURES ----

>> B1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.
/////

struct lock
  {
...
    /* List of threads which try to acquire this lock */
    struct list wait_list;

    /* List element for holding list in thread */
    struct list_elem holding_elem;
...
  };

wait_list is a list element to better order the elements that are trying to acquire the lock (used a lot in priority donation)

holding_elem is a list element that is used by the threads to list which locks they are holding.

/////

struct semaphore_elem
{
  int priority;               /* Associated priority */
};

The priority element in the semaphore_elem thread is used for the cond_comp function that is utilized for ordering the waitingthreads by priority

/////

struct thread
  {...
    /* Priority donation */
    struct list_elem wait_elem;       /* List element for lock thread list */
    struct list holding_lock;         /* List of holding locks */
    struct lock *waiting_lock;        /* Waiting lock */
    int base_priority;                /* Actual priority before donation */
...
}

wait_elem is a list elem that is going to be used by the locks to catalog the threads currently waiting to acquire it
holding_lock is the list to track the locks that are currently being hold by the thread
waiting lock is to represent a lock that the thread might be waiting for
base_priority is the priority before the donation was performed.


>> B2: Explain the data structure used to track priority donation.
>> Use ASCII art to diagram a nested donation.  (Alternately, submit a
>> .png file.)




---- ALGORITHMS ----

>> B3: How do you ensure that the highest priority thread waiting for
>> a lock, semaphore, or condition variable wakes up first?
In the cond_wait, sema_down and lock_acquire functions, the wait list for each of those synchronization elements is filled
using the function list_insert_ordered. This function takes as an argument for the ordering a function defined by us called
priority_comp.


>> B4: Describe the sequence of events when a call to lock_acquire()
>> causes a priority donation.  How is nested donation handled?
When a lock_acquire is called (lets call it L1) the first step is pass through an if statement to check if the lock L1 is
already held by another thread. If it is than a second if statement is used to confirm if the priority of the lock holding
thread (lets call it Tm) is lower than the priority of the current thread(lets call it Th), which would characterize a
priority donation. Afterwards the priority of the Tm is set as the same priority of Th requesting the lock, and then sent
to the nested_donate function. The nested donate function is going to check if there is a "chain"of donations that have to
be performed (if Tm is waiting for another lock). If Tm is not waiting for a lock, the function nested_donate is going to
return without performing any task and Th will be preempted by the function preempt_thread, and Tm will run, subsequently
realeasing the lock. If though, Tm is waiting for  lock, than the same procedure that happened in the function 
lock_acquire happens in the nested_donated function. First it checks if the priority of the lock holder (lets call it Tl) is 
lower than of the Tm, and if it is it performs a priority donation to Tl, and proceeds to cal nested_donate with Tl as an
argument. Than the same procedure is going to happen for Tl, and will continue in succession until one of the nested threads
does not need an outside lock.

>> B5: Describe the sequence of events when lock_release() is called
>> on a lock that a higher-priority thread is waiting for.
First when the lock_release is called, is going to remove the lock from the holding_lock list, to catalog that the calling
thread does not hold the lock anymore. Afterwards its going to be tested if the current thread holds any more locks. If it
does, it is also checked if one of the waiters of said so lock has a higher priority than the current lock holder. If it does
than priority donation is performed. If the current thread does not have any more locks, its priority is set back to
 priority.
---- SYNCHRONIZATION ----

>> B6: Describe a potential race in thread_set_priority(


----) and explain
>> how your implementation avoids it.  Can you use a lock to avoid
>> this race?
During the execution of thread_set_priority, the interruptions are disabled, so there isnt a chance of a timer interrupt
happening during the execution.The race condition in our case is avoided by the use of an ordered list in the modified
thread_yield. We can prove it in the following way:
Lets suppose thread A and B both have priority 28 and thread C has priority 27. A and B both calls thread_set_priority to 
perform the following tasks: change A priority to 29 and B to 26.
If A calls the function first, it will change its priority to 29, and continue to execute as normal. If B calls the function
first, it would change its priority to 26, and immediately yield for A to change its priority and perform its threads tasks.
So in both cases, the thread A would still run its functions first, no matter which thread called thread_set_priority first. 

---- RATIONALE ----

>> B7: Why did you choose this design?  In what ways is it superior to
>> another design you considered?
As it was done with the first part, both students created their own design and chose the best one to submit. The other design that was made proposed the change of sema_up() and cond_signal() function, and for ordering instead of using the 
list_insert_ordered a loop was created where the nested thread inside the semaphores (which were inside the waiters list for 
conditional variables) would have its priority compared with the others on the semaphore waiting list (or in the conditional 
variable semaphore elements list), and the thread with highest priority would be place on the top of the ready_list. A 
function get_ready_list had also been done in that implementation to allow the static list to be accessed from outside the 
thread.c file.
The reason for using this design was for its simplicity. The use of an already existing list_insert_ordered function just 
made it more intuitive to work with, specially for nested donations. Also, in the other design there was many lists that had
to be initiliazed and created on the thread elements, so increasing the size of the threads.

			  ADVANCED SCHEDULER
			  ==================

---- DATA STRUCTURES ----

>> C1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

////

struct thread
  {... 
int nice;                         /* Nice value */
int32_t recent_cpu;               /* Recent cpu for this thread */
...
}
nice is the nice value of the thread.
recent_cpu represents the recent cpu value

////

static int32_t load_avg;
variable for storing the load average

////

#define p 17
#define q 14
#define f (1 << q)

int32_t int_to_fixed (int);
int fixed_to_int_zero (int32_t);
int fixed_to_int_near (int32_t);
int32_t add_fixed (int32_t, int32_t);
int32_t sub_fixed (int32_t, int32_t);
int32_t add_int_fixed (int32_t, int);
int32_t sub_int_fixed (int32_t, int);
int32_t multi_fixed (int32_t, int32_t);
int32_t multi_int_fixed (int32_t, int);
int32_t div_fixed (int32_t, int32_t);
int32_t div_int_fixed (int32_t, int);

p, q and f are defined for the fixed point format
The other variables are used in the fixed point arithmetics to perform the computation for calculating the 
load_avg and recent_cpu
 ALGORITHMS ----

>> C2: Suppose threads A, B, and C have nice values 0, 1, and 2.  Each
>> has a recent_cpu value of 0.  Fill in the table below showing the
>> scheduling decision and the priority and recent_cpu values for each
>> thread after each given number of timer ticks:

timer  recent_cpu    priority   thread
ticks   A   B   C   A   B   C   to run
-----  --  --  --  --  --  --   ------
 0      0   0   0  63  61  59    A
 4      4   0   0  62  61  59    A
 8      8   0   0  61  61  59    A
12     12   0   0  60  61  59    B
16     12   4   0  60  60  59    A
20     16   4   0  59  60  59    B
24     16   8   0  59  59  59    A
28     20   8   0  58  59  59    B
32     20   12  0  58  58  59    C
36     20   12  4  58  58  58    A

>> C3: Did any ambiguities in the scheduler specification make values
>> in the table uncertain?  If so, what rule did you use to resolve
>> them?  Does this match the behavior of your scheduler?
In the cases where the threads had the same priority, the thread with the lowest nice number got to run.

>> C4: How is the way you divided the cost of scheduling between code
>> inside and outside interrupt context likely to affect performance?
The amount of computation performed inside the interrupt is fairly small, since it is just calling of the functions to check to condition for updating the recent_cpu and load_avg. That way the amount of computational work necessary for that will be reduced.

---- RATIONALE ----

>> C5: Briefly critique your design, pointing out advantages and
>> disadvantages in your design choices.  If you were to have extra
>> time to work on this part of the project, how might you choose to
>> refine or improve your design?
Our design makes light use of the interrupt handler, since most functions and routines are defined into a separate header
file. Also we have created a set of functions to specially handle the fixed point arithmetics, therefore we dont need to
keep repeating loops or the same code over again along different functions, making the overall code more succint and easy
to read.


>> C6: The assignment explains arithmetic for fixed-point math in
>> detail, but it leaves it open to you to implement it.  Why did you
>> decide to implement it the way you did?  If you created an
>> abstraction layer for fixed-point math, that is, an abstract data
>> type and/or a set of functions or macros to manipulate fixed-point
>> numbers, why did you do so?  If not, why not?

A set of functions along with a separate header file was created to be able to deal with the fixed points number arithmetic. Reason for that doing was for the better organization and separate the mlfq related functions from the other normal scheduling.


			   SURVEY QUESTIONS
			   ================

Answering these questions is optional, but it will help us improve the
course in future quarters.  Feel free to tell us anything you
want--these questions are just to spur your thoughts.  You may also
choose to respond anonymously in the course evaluations at the end of
the quarter.

>> In your opinion, was this assignment, or any one of the three problems
>> in it, too easy or too hard?  Did it take too long or too little time?

>> Did you find that working on a particular part of the assignment gave
>> you greater insight into some aspect of OS design?

>> Is there some particular fact or hint we should give students in
>> future quarters to help them solve the problems?  Conversely, did you
>> find any of our guidance to be misleading?

>> Do you have any suggestions for the TAs to more effectively assist
>> students, either for future quarters or the remaining projects?

>> Any other comments?
